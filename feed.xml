<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://rotvie.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rotvie.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-24T05:01:55+00:00</updated><id>https://rotvie.github.io/feed.xml</id><title type="html">Ricardo Huaman</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Multi-Armed Bandit Problem</title><link href="https://rotvie.github.io/blog/2026/multi-armed-bandits/" rel="alternate" type="text/html" title="The Multi-Armed Bandit Problem"/><published>2026-02-23T00:00:00+00:00</published><updated>2026-02-23T00:00:00+00:00</updated><id>https://rotvie.github.io/blog/2026/multi-armed-bandits</id><content type="html" xml:base="https://rotvie.github.io/blog/2026/multi-armed-bandits/"><![CDATA[<h2 id="the-multi-armed-bandit-problem">The Multi-Armed Bandit Problem</h2> <p>There are many resources that explain multi-armed bandits as “a smarter A/B test.” While that’s one application, it undersells what’s going on. The bandit problem captures something more fundamental: <strong>how do you make good decisions when every choice teaches you something about the world?</strong> <d-cite key="Slivkins_2019"></d-cite></p> <p>This post builds the full picture — starting from a problem every content creator faces, formalizing it into math, and then walking through three algorithms that solve it in fundamentally different ways.</p> <h2 id="you-just-uploaded-a-video">You Just Uploaded a Video</h2> <p>You’ve spent a week editing a video and it’s ready to publish. But which thumbnail should you use? You’ve prepared <strong>5 candidates</strong> — different compositions, text overlays, color palettes. Each thumbnail will attract a different fraction of viewers who see it in their feed. You want to find the best one as quickly as possible.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-five-thumbnails-480.webp 480w,/assets/img/mab-five-thumbnails-800.webp 800w,/assets/img/mab-five-thumbnails-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-five-thumbnails.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Five thumbnail candidates" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Five thumbnail candidates for the same video. Each has an unknown click-through rate (CTR) — the fraction of viewers who click when they see it in their feed. </div> <p>YouTube actually does something like this internally. When you upload a video, it can show different thumbnails to different viewers and track which one gets the most clicks. But how should it allocate impressions across the candidates?</p> <h3 id="the-naive-approach-equal-split">The naive approach: equal split</h3> <p>The standard approach is a fixed A/B test: split impressions equally across all 5 thumbnails for a few days, then keep the winner. Simple, statistically valid — but during that test period, you’re sending <strong>equal impressions to every thumbnail, including the bad ones</strong>.</p> <p>Say T3 has an 8% CTR and T1 has a 2% CTR. An equal-split test keeps showing T1 to 20% of your audience for the entire test. Every impression spent on T1 instead of T3 is a potential click lost.</p> <h3 id="the-smarter-approach-adapt-as-you-learn">The smarter approach: adapt as you learn</h3> <p>A bandit algorithm starts by exploring all thumbnails, but as it gathers evidence, it <strong>shifts impressions toward the winners</strong> while still occasionally checking the others. Fewer wasted impressions, faster convergence to the best thumbnail.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-ab-vs-bandit-allocation-480.webp 480w,/assets/img/mab-ab-vs-bandit-allocation-800.webp 800w,/assets/img/mab-ab-vs-bandit-allocation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-ab-vs-bandit-allocation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="A/B test vs bandit allocation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An A/B test splits impressions equally (left). A bandit algorithm shifts traffic toward the best-performing thumbnail as evidence accumulates (right). </div> <p>This is the multi-armed bandit problem: balancing <strong>exploiting</strong> what’s working with <strong>exploring</strong> what might work better.</p> <h2 id="formal-definition">Formal Definition</h2> <p>Let’s translate this into math.</p> <p>You have $K$ thumbnail candidates. Each thumbnail $i$ has a true CTR $\theta_i$ that you don’t know. Every time an impression is served (time step $t$), you choose which thumbnail to show (action $a_t$). The viewer either clicks ($r_t = 1$) or scrolls past ($r_t = 0$):</p> \[P(r_t = 1) = \theta_{a_t}\] <p>This is called a <strong>Bernoulli bandit</strong> — each action gives a binary outcome, like a coin flip with unknown bias <d-cite key="Lattimore_Szepesvari_2020"></d-cite>.</p> <h3 id="why-multi-armed-bandit">Why “Multi-Armed Bandit”?</h3> <p>The name comes from slot machines. A “one-armed bandit” is a single slot machine — one lever, steals your money. A “multi-armed bandit” is the problem of facing $K$ machines at once, each with a different hidden payout. Pull an arm, observe the reward, decide which arm to pull next. Your 5 thumbnails are your 5 arms.</p> <h3 id="regret">Regret</h3> <p>We measure performance by <strong>regret</strong> — the total clicks lost by not always showing the best thumbnail:</p> \[\mathcal{L}_T = \sum_{t=1}^{T} (\theta^* - \theta_{a_t}), \quad \theta^* = \max_i \theta_i\] <p>Every impression spent on a suboptimal thumbnail adds some regret. A good algorithm minimizes this.</p> <h3 id="the-estimation-problem">The Estimation Problem</h3> <p>You estimate each thumbnail’s CTR from observed data:</p> \[\hat{Q}_t(a) = \frac{\text{clicks on thumbnail } a}{\text{impressions of thumbnail } a }\] <p>The catch: <strong>early estimates are noisy</strong>. After showing T3 to just 50 viewers, your estimated CTR could be anywhere from 0% to 16%, even if the true CTR is 8%. This noise is what makes the problem hard — and it’s why the three algorithms below take fundamentally different approaches to dealing with it.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-estimation-noise-480.webp 480w,/assets/img/mab-estimation-noise-800.webp 800w,/assets/img/mab-estimation-noise-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-estimation-noise.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Noisy estimates" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> After 50 impressions per thumbnail, error bars are wide and overlap — you can't confidently pick the winner. After 2,000 each, estimates are tight — but that's 10,000 impressions, most wasted on bad thumbnails. </div> <h2 id="algorithm-1-varepsilon-greedy">Algorithm 1: $\varepsilon$-Greedy</h2> <p>The simplest bandit strategy. Most of the time, show the thumbnail with the highest estimated CTR. Occasionally, try a random one:</p> \[a_t = \begin{cases} \displaystyle\operatorname*{argmax}_a \hat{Q}_t(a) &amp; \text{with probability } 1 - \varepsilon \quad (\text{show the current best}) \\ \text{random thumbnail} &amp; \text{with probability } \varepsilon \quad (\text{try something random}) \end{cases}\] <p>Set $\varepsilon = 0.1$ and 10% of impressions go to a random thumbnail.</p> <p><strong>Why it works (sort of):</strong> the random explorations help you discover if a different thumbnail is actually better. Over time, estimates converge and you mostly exploit the true winner.</p> <p><strong>Why it’s limited:</strong> that random 10% is <em>blind</em>. It doesn’t care whether you’ve already confirmed a thumbnail is terrible. After 10,000 impressions, you might have rock-solid evidence that T1 has a 2% CTR, and $\varepsilon$-Greedy will still randomly show it. Those are wasted impressions — viewers who could have been shown T3.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-epsilon-greedy-results-480.webp 480w,/assets/img/mab-epsilon-greedy-results-800.webp 800w,/assets/img/mab-epsilon-greedy-results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-epsilon-greedy-results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Epsilon-greedy results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> $\varepsilon$-Greedy with different exploration rates. Too little exploration ($\varepsilon = 0$) risks getting stuck on a suboptimal thumbnail. Too much ($\varepsilon = 0.3$) wastes impressions on thumbnails already known to be bad. The regret keeps growing linearly — the waste never stops. </div> <h2 id="algorithm-2-upper-confidence-bounds-ucb1">Algorithm 2: Upper Confidence Bounds (UCB1)</h2> <h3 id="why-not-explore-smarter">Why Not Explore Smarter?</h3> <p>The problem with $\varepsilon$-Greedy is that it explores <em>blindly</em>. What if, instead of exploring randomly, we sent impressions specifically to <strong>thumbnails we’re still uncertain about?</strong></p> <h3 id="building-the-intuition">Building the Intuition</h3> <p>Imagine you’ve been running the test and here’s what you know:</p> <table> <thead> <tr> <th>Thumbnail</th> <th>Impressions</th> <th>Clicks</th> <th>Observed CTR</th> <th>How confident are you?</th> </tr> </thead> <tbody> <tr> <td>T1</td> <td>500</td> <td>10</td> <td>2.0%</td> <td>Very confident — it’s bad</td> </tr> <tr> <td>T2</td> <td>500</td> <td>26</td> <td>5.2%</td> <td>Pretty confident</td> </tr> <tr> <td>T3</td> <td>12</td> <td>1</td> <td>8.3%</td> <td><strong>Not confident at all!</strong></td> </tr> <tr> <td>T4</td> <td>500</td> <td>14</td> <td>2.8%</td> <td>Very confident — it’s bad</td> </tr> <tr> <td>T5</td> <td>500</td> <td>21</td> <td>4.2%</td> <td>Pretty confident</td> </tr> </tbody> </table> <p>T3 <em>looks</em> great at 8.3%, but you’ve only shown it 12 times. That could easily be noise — maybe nobody happened to be in the mood to click. Or maybe it really is the best. <strong>You should show it more to find out.</strong></p> <p>Meanwhile, T1 and T4 have been shown 500 times each. You’re very confident they’re bad. There’s no point sending more impressions their way.</p> <p>UCB1 formalizes exactly this intuition. For each thumbnail, it computes a score that combines two things:</p> \[\text{UCB score}(a) = \underbrace{\hat{Q}_t(a)}_{\text{estimated CTR}} + \underbrace{\sqrt{\frac{2 \ln t}{N_t(a)}}}_{\text{how uncertain you are}}\] <p>Then it shows the thumbnail with the highest score.</p> <p>The second term captures your uncertainty about the estimate. It’s large when $N_t(a)$ is small (few impressions → uncertain → explore), and shrinks as you gather data (many impressions → confident → trust the estimate).</p> <h3 id="where-does-the-uncertainty-term-come-from">Where Does the Uncertainty Term Come From?</h3> <p>It comes from <strong>Hoeffding’s Inequality</strong>, a result from probability theory that bounds how far a sample mean can deviate from the true mean <d-cite key="Auer_2002"></d-cite>.</p> <p>Hoeffding tells you: given $N$ observations of a random variable bounded in $[0,1]$, the probability that the true mean exceeds the sample mean by more than $u$ is at most $e^{-2Nu^2}$.</p> <p>If we want to be very sure (probability $\leq t^{-4}$, which is vanishingly small) that our upper bound covers the true CTR, we set $e^{-2Nu^2} = t^{-4}$ and solve:</p> \[e^{-2Nu^2} = t^{-4} \implies u = \sqrt{\frac{2\ln t}{N}}\] <p>That’s the uncertainty term. It’s a statistically rigorous confidence bound — hence the name “Upper Confidence Bound.”</p> <p>The philosophy is sometimes called <strong>“optimism in the face of uncertainty”</strong>: act as if each thumbnail is as good as it <em>plausibly could be</em> given the data. Under-explored thumbnails get the benefit of the doubt.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-ucb-scores-480.webp 480w,/assets/img/mab-ucb-scores-800.webp 800w,/assets/img/mab-ucb-scores-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-ucb-scores.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="UCB score decomposition" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> UCB1 scores for each thumbnail. The uncertainty term (lighter bar) is large for T3 (only 12 impressions) and tiny for well-explored thumbnails. UCB1 sends the next impression to T3 — the thumbnail it knows the least about. </div> <h2 id="algorithm-3-thompson-sampling">Algorithm 3: Thompson Sampling</h2> <h3 id="a-different-way-to-handle-uncertainty">A Different Way to Handle Uncertainty</h3> <p>UCB1 uses a formula to put a number on uncertainty. Thompson Sampling takes a different route: it maintains a <strong>full probability distribution</strong> representing your belief about each thumbnail’s true CTR, and uses randomness to decide what to explore <d-cite key="Chapelle_Li_2011"></d-cite>.</p> <h3 id="beliefs-as-beta-distributions">Beliefs as Beta Distributions</h3> <p>For each thumbnail, you keep a belief: <em>“given everything I’ve observed, what could this thumbnail’s true CTR be?”</em> This belief is a <strong>Beta distribution</strong> $\text{Beta}(\alpha, \beta)$, where:</p> <ul> <li>$\alpha$ = clicks + 1</li> <li>$\beta$ = non-clicks + 1</li> </ul> <p>If T2 has received 100 impressions, 5 clicks, 95 skips: the belief is $\text{Beta}(6, 96)$ — peaked near 5%, fairly narrow. You’re reasonably confident.</p> <p>If T3 has received 3 impressions, 1 click: the belief is $\text{Beta}(2, 3)$ — a very wide distribution spanning nearly the whole range from 0% to 70%. You have almost no idea what T3’s true CTR is.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-beta-distributions-480.webp 480w,/assets/img/mab-beta-distributions-800.webp 800w,/assets/img/mab-beta-distributions-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-beta-distributions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Beta distributions at different stages" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Beta distribution sharpens with data. With few impressions, the belief is wide — the true CTR could be almost anything. With many impressions, the belief narrows to a tight peak near the true value. </div> <h3 id="the-algorithm">The Algorithm</h3> <p>Each time a new viewer arrives:</p> <ol> <li>For each thumbnail, <strong>draw a random value</strong> from its belief distribution.</li> <li>Show the viewer <strong>whichever thumbnail drew the highest value</strong>.</li> <li>Observe click or skip. <strong>Update the belief</strong>: click → $\alpha + 1$, skip → $\beta + 1$.</li> </ol> <p><strong>Why this works:</strong> Thumbnails you’re uncertain about have wide beliefs, so their draws occasionally land very high — they get impressions. Thumbnails you’re confident about have narrow beliefs — draws cluster near the true CTR. If it’s genuinely the best, it consistently “wins the draw.” Over time, all beliefs narrow and the best thumbnail dominates.</p> <p>This property is called <strong>probability matching</strong>: each thumbnail is selected with probability proportional to the probability that it <em>is</em> the best, given what you’ve observed <d-cite key="Russo_2017"></d-cite>.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-thompson-one-round-480.webp 480w,/assets/img/mab-thompson-one-round-800.webp 800w,/assets/img/mab-thompson-one-round-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-thompson-one-round.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="One round of Thompson Sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> One round of Thompson Sampling. Each thumbnail's belief produces a random draw. T3, with its wide belief (only 12 impressions), draws a high value and gets explored — not because of a formula, but because uncertainty naturally produces high samples. </div> <h3 id="beliefs-converge-over-time">Beliefs Converge Over Time</h3> <p>As the algorithm runs, beliefs sharpen. Early on, all distributions are wide and the algorithm explores broadly. Eventually, the best thumbnail’s distribution dominates, and almost all impressions flow there.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-thompson-convergence-480.webp 480w,/assets/img/mab-thompson-convergence-800.webp 800w,/assets/img/mab-thompson-convergence-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-thompson-convergence.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Posterior convergence" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Thompson Sampling posteriors at four time steps. At $t = 0$, all beliefs are flat — "any CTR is equally plausible." By convergence, the best thumbnail (T3, 8% true CTR) has a sharp, dominant peak and receives nearly all impressions. </div> <h2 id="head-to-head-comparison">Head-to-Head Comparison</h2> <p>Now the main event. All three algorithms on the same 5-thumbnail problem (true CTRs: 2%, 3%, 4%, 5%, 8%), over 10,000 impressions, averaged across 30 runs.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mab-comparison-480.webp 480w,/assets/img/mab-comparison-800.webp 800w,/assets/img/mab-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mab-comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Algorithm comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> (a) Cumulative regret: Thompson and UCB1 flatten, while $\varepsilon$-Greedy grows linearly. (b) Log-log plot reveals the growth rate: $O(t)$ for $\varepsilon$-Greedy, $O(\log t)$ for the others. (c) Impression distribution: Thompson concentrates nearly all impressions on the best thumbnail. </div> <p>The key result: $\varepsilon$-Greedy regret grows <strong>linearly</strong> — it never stops wasting impressions. UCB1 and Thompson Sampling grow <strong>logarithmically</strong> — the regret curve flattens as they lock onto the best thumbnail. Thompson typically achieves the lowest total regret.</p> <h2 id="beyond-thumbnails">Beyond Thumbnails</h2> <p>The explore-exploit tradeoff isn’t unique to thumbnails. It appears wherever decisions must be made under uncertainty with limited feedback:</p> <ul> <li><strong>Ad placement</strong>: Which banner to show on a webpage. This is where bandits are most widely deployed in industry.</li> <li><strong>Clinical trials</strong>: Choosing between a proven treatment and a promising experimental one. Bandit algorithms have been proposed for adaptive designs that shift patients toward the more effective treatment as evidence accumulates <d-cite key="Villar_2015"></d-cite>.</li> <li><strong>Recommendation systems</strong>: Should a platform show you content it’s confident you’ll enjoy, or something new that <em>might</em> be a hit?</li> </ul> <p>Even training deep learning models involves this tradeoff. Stochastic Gradient Descent uses mini-batches that introduce noise into gradient estimates — acting as implicit exploration that helps escape local minima. It’s the same “productive randomness” that makes Thompson Sampling work.</p> <h2 id="summary">Summary</h2> <table> <thead> <tr> <th>Algorithm</th> <th>How it explores</th> <th>Regret</th> <th>Key property</th> </tr> </thead> <tbody> <tr> <td>$\varepsilon$-Greedy</td> <td>Randomly — wastes impressions on known-bad thumbnails</td> <td>Linear — $O(T)$</td> <td>Simple but inefficient</td> </tr> <tr> <td>UCB1</td> <td>Targets uncertain thumbnails via a Hoeffding-derived confidence bound</td> <td>Logarithmic — $O(\log T)$</td> <td>Deterministic, theoretically grounded</td> </tr> <tr> <td>Thompson Sampling</td> <td>Draws from belief distributions; uncertain thumbnails can “win” by chance</td> <td>Logarithmic — $O(\log T)$</td> <td>Automatic, typically lowest regret</td> </tr> </tbody> </table> <p>The core insight: <strong>information has value</strong>. Showing a viewer an under-explored thumbnail isn’t wasting that impression — it’s investing in learning which thumbnail is best, which pays off by routing all future viewers better. The question is how to make those investments wisely. $\varepsilon$-Greedy invests randomly. UCB1 and Thompson Sampling invest where it matters.</p> <h2 id="glossary">Glossary</h2> <p><strong>Click-Through Rate (CTR):</strong> The fraction of impressions that result in a click. If 1,000 viewers see a thumbnail and 50 click, the CTR is 5%.</p> <p><strong>Bernoulli Bandit:</strong> A multi-armed bandit where each arm gives a binary reward (0 or 1) with some unknown probability $\theta$.</p> <p><strong>Regret:</strong> The cumulative difference between the reward of the best arm and the arm actually chosen, summed over all time steps. Lower is better.</p> <p><strong>Upper Confidence Bound (UCB):</strong> A score combining the estimated reward with an uncertainty term derived from Hoeffding’s Inequality. Arms with fewer observations get a larger uncertainty term, encouraging exploration.</p> <p><strong>Beta Distribution:</strong> A probability distribution on the interval $[0, 1]$, parameterized by $\alpha$ and $\beta$. In Thompson Sampling, it represents the posterior belief about an arm’s true reward probability.</p> <p><strong>Probability Matching:</strong> The property that each arm is selected with probability equal to the probability it is optimal, given the observed data.</p>]]></content><author><name>Ricardo Huaman</name></author><category term="multi-armed"/><category term="bandits,"/><category term="reinforcement"/><category term="learning,"/><category term="exploration-exploitation,"/><category term="bayesian"/><category term="methods"/><summary type="html"><![CDATA[A visual walkthrough of exploration vs exploitation — from YouTube thumbnails to the algorithms behind smart experimentation]]></summary></entry><entry><title type="html">Langevin Sampling for Diffusion Models</title><link href="https://rotvie.github.io/blog/2026/langevin-sampling-for-diffusion/" rel="alternate" type="text/html" title="Langevin Sampling for Diffusion Models"/><published>2026-02-08T00:00:00+00:00</published><updated>2026-02-08T00:00:00+00:00</updated><id>https://rotvie.github.io/blog/2026/langevin-sampling-for-diffusion</id><content type="html" xml:base="https://rotvie.github.io/blog/2026/langevin-sampling-for-diffusion/"><![CDATA[<h2 id="langevin-sampling-for-diffusion-models">Langevin Sampling for Diffusion Models</h2> <p>There are many resources on the internet that explain diffusion models as a noising and denoising process using a deep learning architecture. Although those are useful, most of them do not paint the whole picture. There’s another component that’s just as important as the deep learning part: the probabilistic framework that makes it all work <d-cite key="DepthFirst_2025"></d-cite>.</p> <h2 id="assigning-probability-values-to-images">Assigning Probability Values to Images</h2> <p>Imagine a black box called the <em>image distribution</em>. This box gives us samples from an underlying distribution — that is, images of all kinds.</p> <p>These images have intrinsic likelihoods, similar to the rolling dice case. An image of a koala and an image of a tiger could have similar probabilities, while a noisy image would have much less likelihood:</p> \[p(\text{Koala}) \approx p(\text{Tiger}) &gt; p(\text{Noise})\] <p>With this in mind, all we need to do is generate samples from this image distribution, right? But first, we need to understand how to generate samples from <em>any</em> distribution using a computer. Let’s start with a simpler case: how do we generate samples from a dice-rolling distribution?</p> <h2 id="challenges-in-sampling-from-probability-distributions">Challenges in Sampling from Probability Distributions</h2> <p>Generating samples from a fair die is the simplest case. There are many libraries already implemented in Python that allow you to do this, such as <code class="language-plaintext highlighter-rouge">random</code>. But how would you implement it from first principles?</p> <p>At first you might think that if we know the PMF of the probability distribution, then we’ll be able to generate samples from it. However, this is not exactly the case — we know how the PMF looks, but actually generating samples from it requires algorithms like the Mersenne Twister or Xorshift random number generators. This sounds kind of intimidating, so let’s look at an alternative approach.</p> <p>Say we have a continuous uniform distribution and we can sample from it. Then:</p> <ol> <li>Sample from the continuous uniform distribution:</li> </ol> \[x \sim \mathcal{U}(0, 1)\] <ol> <li>Threshold on multiples of $1/6$, which gives us the dice roll values:</li> </ol> \[\begin{align*} 0 &lt; x &lt; \frac{1}{6} &amp;: \bullet \\ \frac{1}{6} &lt; x &lt; \frac{2}{6} &amp;: \bullet\bullet \\ &amp;\vdots \\ \frac{5}{6} &lt; x &lt; 1 &amp;: \bullet\bullet\bullet\bullet\bullet\bullet \end{align*}\] <p>Interesting — although both of these are uniform distributions, so the trick feels a bit circular. However, there’s another distribution that, if we could sample from it, would allow us to sample from <em>almost any</em> other distribution.</p> <h2 id="the-probability-density-function-with-two-peaks">The Probability Density Function with Two Peaks</h2> <p>Let’s assume we have a PDF called $p_{\text{twopeaks}}(x)$ — a distribution with two distinct peaks.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ptwopeaks-pdf-480.webp 480w,/assets/img/ptwopeaks-pdf-800.webp 800w,/assets/img/ptwopeaks-pdf-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ptwopeaks-pdf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Two Peaks PDF" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The probability density function \( p_{\text{twopeaks}}(x) \) with two distinct peaks. </div> <p>Also assume we have a function $F_{\text{twopeaks}}(x)$ which, given any input $x_i$, tells us whether we should move left or right to most quickly increase the value of $p_{\text{twopeaks}}(x)$.</p> <p>Our third assumption is that we can generate samples from a normal distribution.</p> <p>With all three assumptions in place, we can begin.</p> <h2 id="langevin-sampling-an-algorithm-that-generates-samples-from-any-probability-distribution">Langevin Sampling: An Algorithm That Generates Samples from Any Probability Distribution</h2> <p><strong>Algorithm: Langevin sampling</strong></p> \[\begin{array}{l} x_0 \sim \mathcal{N}(0, 1) \\ \textbf{for } t = 0 \text{ to } 1000: \\ \quad z_t \sim \mathcal{N}(0, 1) \\ \quad x_{t+1} = x_t + \epsilon F(x_t) + \sqrt{2\epsilon} z_t \\ \textbf{return } x_{1000} \end{array}\] <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langevin-sampling-480.webp 480w,/assets/img/langevin-sampling-800.webp 800w,/assets/img/langevin-sampling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/langevin-sampling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Langevin Sampling Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Langevin Sampling algorithm: iteratively applying gradient-based updates with Gaussian noise to generate samples from a target distribution. </div> <p>The Langevin Sampling algorithm is described in the image above. One iteration in the loop is called a <em>Langevin Update</em>. We iterate thousands of times (say 1,000) to generate a single sample. Then we repeat this entire process many times to build up a histogram that approximates the target distribution.</p> <p>It’s important that we add Gaussian noise at each iteration to prevent the sample from collapsing onto the peak itself. The randomness is essential — it keeps the deterministic gradient ascent from getting stuck.</p> <h2 id="intuition-for-the-score-function">Intuition for the Score Function</h2> <p>Now, let’s look at the $F$ term — the <strong>score function</strong>. It’s defined as:</p> \[F(x_t) = \nabla_x \log p(x_t)\] <p>It’s intuitive that this gradient (derivative) gives us the direction of quickest increase. But why the $\log$?</p> <p>The logarithm amplifies the magnitude of the gradient in low-density regions, allowing us to reach the most probable samples faster. Without it, the gradients in the tails of the distribution would be vanishingly small, making it hard to navigate toward the peaks.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langevin-sampling-gradient-log-480.webp 480w,/assets/img/langevin-sampling-gradient-log-800.webp 800w,/assets/img/langevin-sampling-gradient-log-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/langevin-sampling-gradient-log.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Score Function with Log" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Effect of the logarithm on the score function: amplifying gradients in low-density regions to improve navigation toward the peaks. </div> <p>Now that we have that in place, let’s do an exercise: applying Langevin Sampling to generate samples from our dice-rolling setup. We’ll follow the strategy shown previously — first use Langevin Sampling to sample from a continuous uniform distribution (assuming we’re given a way to sample from a Gaussian distribution to perform the Langevin updates).</p> <h2 id="exercise-a-dice-roll-sampler-from-scratch-using-langevin-sampling">Exercise: A Dice Roll Sampler from Scratch Using Langevin Sampling</h2> <p>First, we need to obtain $F(x)$ for the uniform distribution in the correct sample space (from 0 to 1). If we take only the derivative, it would be zero everywhere inside the interval — not very helpful.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/derivative-uniform-distribution-480.webp 480w,/assets/img/derivative-uniform-distribution-800.webp 800w,/assets/img/derivative-uniform-distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/derivative-uniform-distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Derivative of Uniform Distribution" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The derivative of the uniform distribution is zero everywhere inside the interval, requiring boundary "walls" to keep samples within the valid range. </div> <p>However, this flat derivative could lead samples to drift outside the valid range, so we need to put “walls” that keep us inside the sample space we’re looking for. These walls act like a restoring force that pushes samples back when they wander outside $[0, 1]$.</p> <p>Using this $F$ and the Gaussian distribution, we can generate samples from a uniform distribution. Then we apply the second step: threshold on multiples of $1/6$, and we have a generator of dice rolls built entirely from Langevin dynamics.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/uniform-distribution-from-langevin-480.webp 480w,/assets/img/uniform-distribution-from-langevin-800.webp 800w,/assets/img/uniform-distribution-from-langevin-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/uniform-distribution-from-langevin.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Uniform Distribution from Langevin Sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Generating samples from a uniform distribution using Langevin dynamics, then thresholding to produce dice rolls. </div> <h2 id="a-langevin-approach-to-image-generation">A Langevin Approach to Image Generation</h2> <p>If we were to sample from the PDF $p_{\text{image}}(\mathbf{x})$, where the sample space spans the dimension of all pixels in an image, it becomes much harder to visualize all possible combinations of those pixels. We need some way to obtain the score function of that PDF.</p> <h2 id="visualizing-score-functions-in-higher-dimensions">Visualizing score functions in higher dimensions</h2> <p>The score function can be visualized as a Vector Field that points in the direction of quickest increase of the PDF, with the length of each vector indicating the magnitude (the closer to the peak, the smaller the vector).</p> <p>The major issue for our image PDF is that we don’t have an analytical expression for it:</p> \[\text{for } \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_{1,000,000} \end{bmatrix}, \quad x_i \text{ is pixel } i \text{ of image } \mathbf{x}\] \[\begin{align*} p_{\text{images}}(\mathbf{x}) &amp;= \text{ ?} \\ F_{\text{images}}(\mathbf{x}) &amp;= \nabla_{\mathbf{x}} \log p_{\text{images}}(\mathbf{x}) = \text{ ?} \end{align*}\] <p>In the dice roll case, we <em>did</em> know the PDF of the uniform distribution, so we could compute $F$ by just reasoning about the sample space. But for images, the problem is much harder. However, there’s one advantage we do have: a collection of millions of images from the internet — which are samples from this very PDF.</p> <h2 id="diffusion-models-estimate-unknown-score-functions-from-existing-samples">Diffusion Models Estimate Unknown Score Functions from Existing Samples</h2> <p>A Diffusion Model learns the direction that points toward the closest cluster of good images (with a nuance I’ll address in the final section) — in other words, the score function we’ve been talking about.</p> <p>This connects our Langevin Sampling approach with image generation. If we have the trained model providing the score function and this sampling technique, we can generate plausible images.</p> <h2 id="why-add-more-noise-in-the-denoising-process">Why Add More Noise in the Denoising Process?</h2> <p>Similarly to the $p_{\text{twopeaks}}(x)$ example, if we don’t add noise during the iterative sampling, we get a blurry mess — which could be seen as the average of many images rather than a true sample from the distribution. The noise contributes to two things:</p> <ol> <li><strong>Diversity</strong>: it ensures the generated samples reflect the full probability distribution rather than collapsing to a single mode.</li> <li><strong>Exploration</strong>: it helps escape local optima, combating the short-sightedness of pure gradient ascent.</li> </ol> <h2 id="noise-as-part-of-a-team">Noise as Part of a Team</h2> <p>The diffusion process and the noise can be seen as a two-person team: one logical, one creative. The work of both allows us to get realistic images. The purely logical component can’t generate a realistic image on its own — it needs randomness.</p> <p>This is worth contrasting with Generative Adversarial Networks (GANs), which suffered from a problem known as <em>mode collapse</em>, where outputs became far less diverse than the true distribution.</p> <h2 id="parallels-with-stochastic-gradient-descent">Parallels with Stochastic Gradient Descent</h2> <p>Stochastic Gradient Descent does something remarkably similar. Instead of finding a peak (like Langevin Sampling), it finds a minimum — but the principle is the same. The “noise” in SGD isn’t added explicitly; it’s a natural consequence of using mini-batches of data, which makes the gradient inherently noisy. This method works incredibly well and is the foundation of deep learning during training. Langevin Sampling just extends this same idea of productive randomness to test time.</p> <h2 id="final-nuances">Final Nuances</h2> <p>There are a few important details I glossed over for the sake of clarity:</p> <ul> <li> <p><strong>Annealed Langevin Sampling</strong>: The model actually learns the score of the <em>noised</em> distribution at each noise level. Why? Because if we were to start from a noise sample and try to follow the score of the true (un-noised) distribution, it would be too sparse — you’d get lost. Instead, we add noise to give the score function meaningful signal, and then gradually reduce the noise level, annealing from high to low noise during sampling.</p> </li> <li> <p><strong>Noise prediction ≈ Score estimation</strong>: During diffusion model training, the model learns to predict the added noise, and it’s not immediately obvious how this connects to the score function. Proving this equivalence is a topic for another post, but the short version is that predicting the noise is mathematically equivalent to estimating the score.</p> </li> <li> <p><strong>Why does Langevin Sampling work at all?</strong> This question leans into statistical physics and stochastic calculus. The theoretical justification comes from the theory of Langevin dynamics and its connection to the Fokker-Planck equation. If you’re curious, that’s where to dig deeper.</p> </li> </ul> <h2 id="glossary">Glossary</h2> <p><strong>Probability Mass Function (PMF):</strong> For a discrete random variable $X$, the PMF $p_X(x)$ gives the probability that $X$ takes the value $x$:</p> \[p_X(x) = P(X = x)\] <p>where $\sum_{x \in \mathcal{X}} p_X(x) = 1$.</p> <p><strong>Probability Density Function (PDF):</strong> For a continuous random variable $X$, the PDF $f_X(x)$ describes the probability density at $x$. The probability that $X$ falls in an interval $[a, b]$ is:</p> \[P(a \leq X \leq b) = \int_a^b f_X(x) \, dx\] <p>where $\int_{-\infty}^{\infty} f_X(x) \, dx = 1$.</p> <p><strong>Score Function:</strong> The gradient of the log-probability density, $\nabla_x \log p(x)$. It points in the direction that most quickly increases the probability density.</p> <p><strong>Langevin Sampling:</strong> An iterative algorithm that generates samples from a target distribution using only the score function and Gaussian noise.</p> <p><strong>Mode Collapse:</strong> A failure mode in GANs where the generator produces only a small subset of the possible outputs, lacking diversity.</p>]]></content><author><name>Ricardo Huaman</name></author><category term="diffusion"/><category term="models,"/><category term="langevin"/><category term="sampling,"/><category term="generative"/><category term="models"/><summary type="html"><![CDATA[An overview of Langevin Sampling for Diffusion Models]]></summary></entry><entry><title type="html">My Journey Learning Japanese</title><link href="https://rotvie.github.io/blog/2025/my-journey-learning-japanese/" rel="alternate" type="text/html" title="My Journey Learning Japanese"/><published>2025-11-29T00:00:00+00:00</published><updated>2025-11-29T00:00:00+00:00</updated><id>https://rotvie.github.io/blog/2025/my-journey-learning-japanese</id><content type="html" xml:base="https://rotvie.github.io/blog/2025/my-journey-learning-japanese/"><![CDATA[<h2 id="my-childhood">My Childhood</h2> <p>Looking back, I think I’ve always had an interest in languages. The idea of communicating in ways that other people couldn’t easily understand fascinated me. It all started when I was around 8 years old. I remember learning an alternative symbol for each letter of the alphabet and writing secret messages with those. It was quite fun, but it had the shortcoming that I was the only one who knew that code, so I couldn’t share my ideas with others.</p> <p>A few years later, when I was 12, I learned about Braille and finally understood what the symbols on the buttons of elevators meant. But most importantly, I appreciated the cleverness in such a system that allowed blind people to read. During the following years, I realized that learning a new language would be much more useful, so I set that as my goal.</p> <h2 id="getting-started">Getting Started</h2> <p>There were many options. I tried some of them using Duolingo and similar apps. I studied French, Portuguese, German, Italian, Esperanto, among other languages, but of all of them, Japanese was the one that really got my attention. It seemed complex, yet Japanese culture was so widespread that it shouldn’t be too hard, I thought—how naive I was then.</p> <p>In December 2014, I decided that I was going to master Japanese. At first, I started reading a book of common phrases in Japanese with its translation in Spanish. However, that alone wasn’t going to take me far, so I began looking for a concise grammar book. That’s when I came across <a href="https://www.guidetojapanese.org/grammar_guide.pdf">Japanese Grammar Guide</a> by Tae Kim.</p> <h2 id="first-attempt">First Attempt</h2> <p>English was a compulsory subject in my school, so I knew the basics, but I was still far from being fluent. Nonetheless, if I wanted to read that Tae Kim textbook, I had to force myself to read English.</p> <p>In 2015, after learning Hiragana with the help of mnemonics, I began reading the book. During that process, I had to look up many English words and sentence structures that now are obvious to me. I think the most noticeable change then was how much my English improved—more than my Japanese, even.</p> <p>Around that time, I also liked to use a website called <a href="https://en.wikipedia.org/wiki/Lang-8.com">Lang-8</a> where you could post paragraphs in the language you were learning for native speakers to correct. I also bought <a href="https://en.wikipedia.org/wiki/Remembering_the_Kanji">Remembering the Kanji</a> by James Heisig, but never really read it because of bad reviews I saw immediately afterwards (I kind of regret not reading it then).</p> <h2 id="getting-off-track">Getting Off Track</h2> <p>I didn’t finish reading Japanese Grammar Guide. I got sidetracked by English content on the internet, but at least I had the general idea of how Japanese worked. A few years passed and my English understanding improved gradually. I knew some Japanese already too, so I thought I would get better if I kept reviewing a little here and there. That never happened.</p> <p>I still had the intention of becoming fluent in another language. Japanese was at the top of my list, but I wasn’t sure how to tackle a language that looked so different from the others. At the end of 2019, Matt vs Japan Youtube channel finally clarified the strategy for me.</p> <h2 id="finding-direction">Finding Direction</h2> <p>The approach to language learning that he used to learn Japanese made sense to me. He said that the most important thing was to focus on getting input in the target language through reading and listening. I reflected upon the way I learned English and everything clicked for me—immersion in the language is crucial.</p> <p>What about Kanji? There are thousands of those Chinese characters. Matt’s solution was the book Remembering the Kanji along with a software for spaced repetition called <a href="https://apps.ankiweb.net/">Anki</a>.</p> <h2 id="second-attempt">Second Attempt</h2> <p>Once again, I was all-in with Japanese, but this time I had a plan. The first few months of 2020, I was learning to write Kanji at a rate of 25 new characters every day using mnemonics and Anki. When I got to a thousand, I stopped to prioritize watching Japanese content. Maybe I should have continued until I was done with the 2,200 Kanji in the book, but I was excited to start watching actual Japanese and expand my vocabulary.</p> <p>After a couple of months of consuming content in Japanese, I returned to Kanji and finished the book at last.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/my-japanese-handwriting-480.webp 480w,/assets/img/my-japanese-handwriting-800.webp 800w,/assets/img/my-japanese-handwriting-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/my-japanese-handwriting.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="where-im-at">Where I’m At</h2> <p>In July 2022 I passed the <a href="https://www.linkedin.com/posts/ricardo-jesus-huaman-kemper_certificado-jlpt-n2-activity-6988881391144812544-pfup?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAACZjz_wBVCnBVO8PKF0qAZFPBmbqVUDky4I">JLPT N2</a> exam after doing immersion for around 2 years at that point. I’ve also created a <a href="https://github.com/Rotvie/ajatt">repository</a> where I share the tools and resources I’ve found most useful for language learning. Now I’m just keeping the routine of learning new words through a technique called <em>Sentence Mining</em> and trying to immerse myself in the language as much as possible, although it’s been harder lately because of other priorities.</p> <p>I omitted tons of details in my story, mainly because I don’t remember them well or because they’re kind of irrelevant. I wanted to write this down before I forget even further.</p> <h2 id="final-thoughts">Final Thoughts</h2> <p>I think learning Japanese has been and continues to be an enriching experience that has shaped who I am now. I encourage everyone who is thinking about learning a new language to give it a try—not just for the sake of a better resume or a job opportunity. Don’t get me wrong, I recognize those are great things too. Just don’t underestimate the impact it can have on one’s understanding of different cultures and perspectives.</p> <p>Best of luck with your studies!</p>]]></content><author><name>Ricardo Huaman</name></author><category term="japanese,"/><category term="language"/><category term="learning"/><summary type="html"><![CDATA[My personal journey learning Japanese]]></summary></entry><entry><title type="html">Setting Up and Running microROS on the Raspberry Pi Pico</title><link href="https://rotvie.github.io/blog/2023/uros/" rel="alternate" type="text/html" title="Setting Up and Running microROS on the Raspberry Pi Pico"/><published>2023-06-08T00:00:00+00:00</published><updated>2023-06-08T00:00:00+00:00</updated><id>https://rotvie.github.io/blog/2023/uros</id><content type="html" xml:base="https://rotvie.github.io/blog/2023/uros/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In the rapidly evolving world of robotics and embedded systems, microROS is a game-changer. It brings the capabilities of ROS (Robot Operating System) 2 to microcontrollers, facilitating the development of robust and interactive systems. The Raspberry Pi Pico, with its powerful RP2040 chip, provides an excellent platform for such applications.</p> <p>This tutorial will guide you through setting up and running microROS on the Raspberry Pi Pico. By the end of this tutorial, you’ll be equipped to leverage the power of microROS on your Raspberry Pi Pico <d-cite key="RoboFoundry_2022"></d-cite> <d-cite key="ubuntuGettingStarted"></d-cite> <d-cite key="micro-ros-agent-ros"></d-cite> <d-cite key="micro-ros-agent-github"></d-cite>.</p> <p>For the corresponding <strong>YouTube tutorial</strong>, click <a href="https://youtu.be/_4aLzvG6dfw?si=oyEmNZsfEt8569Nv">here</a>.</p> <hr/> <h2 id="setting-up-the-environment">Setting up the Environment</h2> <p>Install the necessary dependencies with the following command:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>cmake g++ gcc-arm-none-eabi doxygen libnewlib-arm-none-eabi git python3 build-essential
</code></pre></div></div> <p>Configure the environment:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"export PICO_TOOLCHAIN_PATH=..."</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">echo</span> <span class="s2">"export PICO_SDK_PATH=</span><span class="nv">$HOME</span><span class="s2">/micro_ros_ws/src/pico-sdk"</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">source</span> ~/.bashrc
</code></pre></div></div> <h2 id="pico-sdk-and-microros">Pico-SDK and microROS</h2> <p>Create the workspace and clone the required repositories:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> ~/micro_ros_ws/src
<span class="nb">cd</span> ~/micro_ros_ws/src
git clone <span class="nt">--recurse-submodules</span> https://github.com/raspberrypi/pico-sdk.git
git clone https://github.com/Rotvie/micro_ros_raspberrypi_pico_sdk.git
</code></pre></div></div> <h2 id="compilation-and-loading">Compilation and Loading</h2> <p>Compile the microROS code to generate the UF2 file, which will be loaded onto the Raspberry Pi Pico:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>micro_ros_raspberrypi_pico_sdk
<span class="nb">mkdir </span>build
<span class="nb">cd </span>build
cmake ..
make
</code></pre></div></div> <p>To load the UF2 file, connect the Pico while holding the BOOTSEL button, then run:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cp </span>build/pico_micro_ros_example.uf2 /media/<span class="nv">$USER</span>/RPI-RP2
</code></pre></div></div> <h2 id="configuring-the-microros-agent">Configuring the microROS agent</h2> <p>Install and configure the microROS agent:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>snap <span class="nb">install </span>micro-ros-agent
<span class="nb">sudo </span>snap <span class="nb">set </span>core experimental.hotplug<span class="o">=</span><span class="nb">true
sudo </span>systemctl restart snapd
snap interface serial-port
snap connect micro-ros-agent:serial-port snapd:pico
</code></pre></div></div> <h2 id="running-the-microros-example-demo">Running the microROS example demo</h2> <p>To start the microROS node, run:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>micro-ros-agent serial <span class="nt">--dev</span> /dev/ttyACM0 <span class="nv">baudrate</span><span class="o">=</span>115200
</code></pre></div></div> <p>Check the available topics with:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 topic list
</code></pre></div></div> <h2 id="connecting-the-pico-with-a-raspberry-pi-instead-of-a-pc">Connecting the Pico with a Raspberry Pi Instead of a PC</h2> <p>For a Raspberry Pi setup, follow the instructions up to the Configuring the microROS agent section. However, if you encounter an error when installing the micro-ros-agent with snap (e.g., <code class="language-plaintext highlighter-rouge">error: snap "micro-ros-agent" is not available on stable for this architecture (arm64)</code>), follow these steps:</p> <ol> <li>Set up a new ROS 2 workspace for the microROS agent:</li> </ol> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> /opt/ros/<span class="nv">$ROS_DISTRO</span>/setup.bash
<span class="nb">mkdir </span>uros_ws <span class="o">&amp;&amp;</span> <span class="nb">cd </span>uros_ws
git clone <span class="nt">-b</span> <span class="nv">$ROS_DISTRO</span> https://github.com/micro-ROS/micro_ros_setup.git src/micro_ros_setup
rosdep update <span class="o">&amp;&amp;</span> rosdep <span class="nb">install</span> <span class="nt">--from-paths</span> src <span class="nt">--ignore-src</span> <span class="nt">-y</span>
colcon build
<span class="nb">source install</span>/local_setup.bash
</code></pre></div></div> <ol> <li>Build the microROS Agent manually:</li> </ol> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 run micro_ros_setup create_agent_ws.sh
ros2 run micro_ros_setup build_agent.sh
<span class="nb">source install</span>/local_setup.sh
</code></pre></div></div> <ol> <li>Start the microROS Agent:</li> </ol> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ros2 run micro_ros_agent micro_ros_agent <span class="nt">--dev</span> /dev/ttyACM0 <span class="nv">baudrate</span><span class="o">=</span>115200
</code></pre></div></div>]]></content><author><name>Ricardo Huaman</name></author><category term="ros,"/><category term="microROS,"/><category term="microcontroller,"/><category term="robotics"/><summary type="html"><![CDATA[A tutorial on how to get microROS and ROS2 working together]]></summary></entry></feed>