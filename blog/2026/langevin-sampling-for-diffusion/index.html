<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Langevin Sampling for Diffusion Models | Ricardo Huaman </title> <meta name="author" content="Ricardo J. Huaman K."> <meta name="description" content="An overview of Langevin Sampling for Diffusion Models"> <meta name="keywords" content="robotics, ai, computer science, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rotvie.github.io/blog/2026/langevin-sampling-for-diffusion/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Langevin Sampling for Diffusion Models",
            "description": "An overview of Langevin Sampling for Diffusion Models",
            "published": "February 08, 2026",
            "authors": [
              
              {
                "author": "Ricardo Huaman",
                "authorURL": "https://rotvie.github.io/",
                "affiliations": [
                  {
                    "name": "Independent",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ricardo Huaman </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Langevin Sampling for Diffusion Models</h1> <p>An overview of Langevin Sampling for Diffusion Models</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="langevin-sampling-for-diffusion-models">Langevin Sampling for Diffusion Models</h2> <p>There are many resources on the internet that explain diffusion models as a noising and denoising process using a deep learning architecture. Although those are useful, most of them do not paint the whole picture. There’s another component that’s just as important as the deep learning part: the probabilistic framework that makes it all work <d-cite key="DepthFirst_2025"></d-cite>.</p> <h2 id="assigning-probability-values-to-images">Assigning Probability Values to Images</h2> <p>Imagine a black box called the <em>image distribution</em>. This box gives us samples from an underlying distribution — that is, images of all kinds.</p> <p>These images have intrinsic likelihoods, similar to the rolling dice case. An image of a koala and an image of a tiger could have similar probabilities, while a noisy image would have much less likelihood:</p> \[p(\text{Koala}) \approx p(\text{Tiger}) &gt; p(\text{Noise})\] <p>With this in mind, all we need to do is generate samples from this image distribution, right? But first, we need to understand how to generate samples from <em>any</em> distribution using a computer. Let’s start with a simpler case: how do we generate samples from a dice-rolling distribution?</p> <h2 id="challenges-in-sampling-from-probability-distributions">Challenges in Sampling from Probability Distributions</h2> <p>Generating samples from a fair die is the simplest case. There are many libraries already implemented in Python that allow you to do this, such as <code class="language-plaintext highlighter-rouge">random</code>. But how would you implement it from first principles?</p> <p>At first you might think that if we know the PMF of the probability distribution, then we’ll be able to generate samples from it. However, this is not exactly the case — we know how the PMF looks, but actually generating samples from it requires algorithms like the Mersenne Twister or Xorshift random number generators. This sounds kind of intimidating, so let’s look at an alternative approach.</p> <p>Say we have a continuous uniform distribution and we can sample from it. Then:</p> <ol> <li>Sample from the continuous uniform distribution:</li> </ol> \[x \sim \mathcal{U}(0, 1)\] <ol> <li>Threshold on multiples of $1/6$, which gives us the dice roll values:</li> </ol> \[\begin{align*} 0 &lt; x &lt; \frac{1}{6} &amp;: \bullet \\ \frac{1}{6} &lt; x &lt; \frac{2}{6} &amp;: \bullet\bullet \\ &amp;\vdots \\ \frac{5}{6} &lt; x &lt; 1 &amp;: \bullet\bullet\bullet\bullet\bullet\bullet \end{align*}\] <p>Interesting — although both of these are uniform distributions, so the trick feels a bit circular. However, there’s another distribution that, if we could sample from it, would allow us to sample from <em>almost any</em> other distribution.</p> <h2 id="the-probability-density-function-with-two-peaks">The Probability Density Function with Two Peaks</h2> <p>Let’s assume we have a PDF called $p_{\text{twopeaks}}(x)$ — a distribution with two distinct peaks.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ptwopeaks-pdf-480.webp 480w,/assets/img/ptwopeaks-pdf-800.webp 800w,/assets/img/ptwopeaks-pdf-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ptwopeaks-pdf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Two Peaks PDF" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The probability density function \( p_{\text{twopeaks}}(x) \) with two distinct peaks. </div> <p>Also assume we have a function $F_{\text{twopeaks}}(x)$ which, given any input $x_i$, tells us whether we should move left or right to most quickly increase the value of $p_{\text{twopeaks}}(x)$.</p> <p>Our third assumption is that we can generate samples from a normal distribution.</p> <p>With all three assumptions in place, we can begin.</p> <h2 id="langevin-sampling-an-algorithm-that-generates-samples-from-any-probability-distribution">Langevin Sampling: An Algorithm That Generates Samples from Any Probability Distribution</h2> <p><strong>Algorithm: Langevin sampling</strong></p> \[\begin{array}{l} x_0 \sim \mathcal{N}(0, 1) \\ \textbf{for } t = 0 \text{ to } 1000: \\ \quad z_t \sim \mathcal{N}(0, 1) \\ \quad x_{t+1} = x_t + \epsilon F(x_t) + \sqrt{2\epsilon} z_t \\ \textbf{return } x_{1000} \end{array}\] <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langevin-sampling-480.webp 480w,/assets/img/langevin-sampling-800.webp 800w,/assets/img/langevin-sampling-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/langevin-sampling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Langevin Sampling Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Langevin Sampling algorithm: iteratively applying gradient-based updates with Gaussian noise to generate samples from a target distribution. </div> <p>The Langevin Sampling algorithm is described in the image above. One iteration in the loop is called a <em>Langevin Update</em>. We iterate thousands of times (say 1,000) to generate a single sample. Then we repeat this entire process many times to build up a histogram that approximates the target distribution.</p> <p>It’s important that we add Gaussian noise at each iteration to prevent the sample from collapsing onto the peak itself. The randomness is essential — it keeps the deterministic gradient ascent from getting stuck.</p> <h2 id="intuition-for-the-score-function">Intuition for the Score Function</h2> <p>Now, let’s look at the $F$ term — the <strong>score function</strong>. It’s defined as:</p> \[F(x_t) = \nabla_x \log p(x_t)\] <p>It’s intuitive that this gradient (derivative) gives us the direction of quickest increase. But why the $\log$?</p> <p>The logarithm amplifies the magnitude of the gradient in low-density regions, allowing us to reach the most probable samples faster. Without it, the gradients in the tails of the distribution would be vanishingly small, making it hard to navigate toward the peaks.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langevin-sampling-gradient-log-480.webp 480w,/assets/img/langevin-sampling-gradient-log-800.webp 800w,/assets/img/langevin-sampling-gradient-log-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/langevin-sampling-gradient-log.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Score Function with Log" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Effect of the logarithm on the score function: amplifying gradients in low-density regions to improve navigation toward the peaks. </div> <p>Now that we have that in place, let’s do an exercise: applying Langevin Sampling to generate samples from our dice-rolling setup. We’ll follow the strategy shown previously — first use Langevin Sampling to sample from a continuous uniform distribution (assuming we’re given a way to sample from a Gaussian distribution to perform the Langevin updates).</p> <h2 id="exercise-a-dice-roll-sampler-from-scratch-using-langevin-sampling">Exercise: A Dice Roll Sampler from Scratch Using Langevin Sampling</h2> <p>First, we need to obtain $F(x)$ for the uniform distribution in the correct sample space (from 0 to 1). If we take only the derivative, it would be zero everywhere inside the interval — not very helpful.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/derivative-uniform-distribution-480.webp 480w,/assets/img/derivative-uniform-distribution-800.webp 800w,/assets/img/derivative-uniform-distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/derivative-uniform-distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Derivative of Uniform Distribution" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The derivative of the uniform distribution is zero everywhere inside the interval, requiring boundary "walls" to keep samples within the valid range. </div> <p>However, this flat derivative could lead samples to drift outside the valid range, so we need to put “walls” that keep us inside the sample space we’re looking for. These walls act like a restoring force that pushes samples back when they wander outside $[0, 1]$.</p> <p>Using this $F$ and the Gaussian distribution, we can generate samples from a uniform distribution. Then we apply the second step: threshold on multiples of $1/6$, and we have a generator of dice rolls built entirely from Langevin dynamics.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/uniform-distribution-from-langevin-480.webp 480w,/assets/img/uniform-distribution-from-langevin-800.webp 800w,/assets/img/uniform-distribution-from-langevin-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/uniform-distribution-from-langevin.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Uniform Distribution from Langevin Sampling" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Generating samples from a uniform distribution using Langevin dynamics, then thresholding to produce dice rolls. </div> <h2 id="a-langevin-approach-to-image-generation">A Langevin Approach to Image Generation</h2> <p>If we were to sample from the PDF $p_{\text{image}}(\mathbf{x})$, where the sample space spans the dimension of all pixels in an image, it becomes much harder to visualize all possible combinations of those pixels. We need some way to obtain the score function of that PDF.</p> <h2 id="visualizing-score-functions-in-higher-dimensions">Visualizing score functions in higher dimensions</h2> <p>The score function can be visualized as a Vector Field that points in the direction of quickest increase of the PDF, with the length of each vector indicating the magnitude (the closer to the peak, the smaller the vector).</p> <p>The major issue for our image PDF is that we don’t have an analytical expression for it:</p> \[\text{for } \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_{1,000,000} \end{bmatrix}, \quad x_i \text{ is pixel } i \text{ of image } \mathbf{x}\] \[\begin{align*} p_{\text{images}}(\mathbf{x}) &amp;= \text{ ?} \\ F_{\text{images}}(\mathbf{x}) &amp;= \nabla_{\mathbf{x}} \log p_{\text{images}}(\mathbf{x}) = \text{ ?} \end{align*}\] <p>In the dice roll case, we <em>did</em> know the PDF of the uniform distribution, so we could compute $F$ by just reasoning about the sample space. But for images, the problem is much harder. However, there’s one advantage we do have: a collection of millions of images from the internet — which are samples from this very PDF.</p> <h2 id="diffusion-models-estimate-unknown-score-functions-from-existing-samples">Diffusion Models Estimate Unknown Score Functions from Existing Samples</h2> <p>A Diffusion Model learns the direction that points toward the closest cluster of good images (with a nuance I’ll address in the final section) — in other words, the score function we’ve been talking about.</p> <p>This connects our Langevin Sampling approach with image generation. If we have the trained model providing the score function and this sampling technique, we can generate plausible images.</p> <h2 id="why-add-more-noise-in-the-denoising-process">Why Add More Noise in the Denoising Process?</h2> <p>Similarly to the $p_{\text{twopeaks}}(x)$ example, if we don’t add noise during the iterative sampling, we get a blurry mess — which could be seen as the average of many images rather than a true sample from the distribution. The noise contributes to two things:</p> <ol> <li> <strong>Diversity</strong>: it ensures the generated samples reflect the full probability distribution rather than collapsing to a single mode.</li> <li> <strong>Exploration</strong>: it helps escape local optima, combating the short-sightedness of pure gradient ascent.</li> </ol> <h2 id="noise-as-part-of-a-team">Noise as Part of a Team</h2> <p>The diffusion process and the noise can be seen as a two-person team: one logical, one creative. The work of both allows us to get realistic images. The purely logical component can’t generate a realistic image on its own — it needs randomness.</p> <p>This is worth contrasting with Generative Adversarial Networks (GANs), which suffered from a problem known as <em>mode collapse</em>, where outputs became far less diverse than the true distribution.</p> <h2 id="parallels-with-stochastic-gradient-descent">Parallels with Stochastic Gradient Descent</h2> <p>Stochastic Gradient Descent does something remarkably similar. Instead of finding a peak (like Langevin Sampling), it finds a minimum — but the principle is the same. The “noise” in SGD isn’t added explicitly; it’s a natural consequence of using mini-batches of data, which makes the gradient inherently noisy. This method works incredibly well and is the foundation of deep learning during training. Langevin Sampling just extends this same idea of productive randomness to test time.</p> <h2 id="final-nuances">Final Nuances</h2> <p>There are a few important details I glossed over for the sake of clarity:</p> <ul> <li> <p><strong>Annealed Langevin Sampling</strong>: The model actually learns the score of the <em>noised</em> distribution at each noise level. Why? Because if we were to start from a noise sample and try to follow the score of the true (un-noised) distribution, it would be too sparse — you’d get lost. Instead, we add noise to give the score function meaningful signal, and then gradually reduce the noise level, annealing from high to low noise during sampling.</p> </li> <li> <p><strong>Noise prediction ≈ Score estimation</strong>: During diffusion model training, the model learns to predict the added noise, and it’s not immediately obvious how this connects to the score function. Proving this equivalence is a topic for another post, but the short version is that predicting the noise is mathematically equivalent to estimating the score.</p> </li> <li> <p><strong>Why does Langevin Sampling work at all?</strong> This question leans into statistical physics and stochastic calculus. The theoretical justification comes from the theory of Langevin dynamics and its connection to the Fokker-Planck equation. If you’re curious, that’s where to dig deeper.</p> </li> </ul> <h2 id="glossary">Glossary</h2> <p><strong>Probability Mass Function (PMF):</strong> For a discrete random variable $X$, the PMF $p_X(x)$ gives the probability that $X$ takes the value $x$:</p> \[p_X(x) = P(X = x)\] <p>where $\sum_{x \in \mathcal{X}} p_X(x) = 1$.</p> <p><strong>Probability Density Function (PDF):</strong> For a continuous random variable $X$, the PDF $f_X(x)$ describes the probability density at $x$. The probability that $X$ falls in an interval $[a, b]$ is:</p> \[P(a \leq X \leq b) = \int_a^b f_X(x) \, dx\] <p>where $\int_{-\infty}^{\infty} f_X(x) \, dx = 1$.</p> <p><strong>Score Function:</strong> The gradient of the log-probability density, $\nabla_x \log p(x)$. It points in the direction that most quickly increases the probability density.</p> <p><strong>Langevin Sampling:</strong> An iterative algorithm that generates samples from a target distribution using only the score function and Gaussian noise.</p> <p><strong>Mode Collapse:</strong> A failure mode in GANs where the generator produces only a small subset of the possible outputs, lacking diversity.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/langevin-sampling-for-diffusion.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/my-journey-learning-japanese/">My Journey Learning Japanese</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/uros/">Setting Up and Running microROS on the Raspberry Pi Pico</a> </li> <br> <br> <div id="giscus_thread"> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ricardo J. Huaman K.. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 09, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>